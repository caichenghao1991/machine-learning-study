{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 线性回归（linear regression）\n",
    "定义回归模型（model）: $f_\\theta(x)=\\theta_0+\\theta_1x$  $\\;\\;\\;\\;\\;\\;$ 【一次线性模型，$\\theta_0$为斜率（slope）， $\\theta_0$为截距（intercept)】  \n",
    "其中参数（parameter）为 $\\theta$， x 为输入变量或输入特征(input variable/features)， y 为输出变量或目标变量(output variable/target variable)  \n",
    "目标函数（cost/objective function）: $$E(\\theta)=\\frac{1}{2}\\sum^n_{i=1}\\,(y^{(i)}-f_\\theta(x^{(i)}))^2$$\n",
    "    **梯度下降法**（gradient descent）: 使用所有训练数据的误差令更新参数(求最小值时，原参数值减去偏微分乘以学习率)。对于多变量函数不能用普通的微分，而要用偏微分。其中 $\\eta$ 为学习率（learning rate）。存在耗时长，容易陷入局部最优解的缺点。\n",
    "    $$\\theta_0:=\\theta_0-\\eta\\frac{\\partial E}{\\partial \\theta_0}, \\;\\;\\;\\;\\; \\theta_1:=\\theta_1-\\eta\\frac{\\partial E}{\\partial \\theta_1}$$\n",
    "    令 $u=E(\\theta), \\;\\;\\;\\;\\; v=f_\\theta(x), \\;\\;\\;\\;\\; \\frac{\\partial u}{\\partial \\theta_0}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_0},$\n",
    "    $$\\frac{\\partial u}{\\partial v}=\\frac{\\partial }{\\partial v}(\\frac{1}{2}\\sum^n_{i=1}\\,(y^{(i)}-v)^2)=\\frac{1}{2}\\sum^n_{i=1}\\,(\\frac{\\partial }{\\partial v}\n",
    "    (y^{(i)^2}-2y^{(i)}v+v^2))=\\sum^n_{i=1}\\,(v-y^{(i)}),\\;\\;\\;\\;\\;  \\frac{\\partial v}{\\partial \\theta_0}=\\frac{\\partial }{\\partial \\theta_0}(\\theta_0+\\theta_1 x)=1$$\n",
    "    $$\\frac{\\partial u}{\\partial \\theta_0}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_0}=\\sum^n_{i=1}\\,(v-y^{(i)})=\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})$$\n",
    "    同理， $\\frac{\\partial v}{\\partial \\theta_1}=\\frac{\\partial }{\\partial \\theta_1}(\\theta_0+\\theta_1 x)=x, \\;\\;\\;\\;\\; \\frac{\\partial u}{\\partial \\theta_1}=$\n",
    "    $\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_1}=\\sum^n_{i=1}\\,(v-y^{(i)})\\cdot x^{(i)}=\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x^{(i)}$ \n",
    "    $$\\theta_0:=\\theta_0-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)}), \\;\\;\\;\\;\\; \\theta_1:=\\theta_1-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x^{(i)}$$\n",
    "\n",
    "**多项式回归**：$$f_\\theta(x)=\\theta_0+\\theta_1x+\\theta_2x^2+\\theta_3x^3+...+\\theta_n x^n  \\;\\;\\;\\;\\;【多项式模型】$$       \n",
    "$$同理，\\theta_j:=\\theta_j-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x^{(i)^j}$$\n",
    "**多重回归**(多个变量)：\n",
    "\n",
    "$$f_\\theta(x)=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n=\\bm \\theta^T \\bm x, \\;\\;\\;\\;\\; 其中 \\; \\bm \\theta=\\begin{bmatrix}\n",
    "\\theta_0\\\\ \\theta_1\\\\ \\theta_2\\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\;\\;\\; \\bm x=\\begin{bmatrix}x_0\\\\ x_1\\\\ x_2\\\\ \\vdots \\\\ x_n \\end{bmatrix}\n",
    "\\;\\;\\; (x_0=1) $$\n",
    "$$\\frac{\\partial u}{\\partial \\theta_j}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_j},  \\;\\;\\;\\;\\;\n",
    "\\frac{\\partial v}{\\partial \\theta_j}=\\frac{\\partial }{\\partial \\theta_j}(\\bm \\theta^T\\bm x)=x_j$$\n",
    "$$\\theta_j:=\\theta_j-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$\n",
    "**随机梯度下降法**（sochastic gradient descent）：使用随机的一个训练数据(索引为 k)更新参数。更新快，不易陷入局部最优解。\n",
    "$$\\theta_j:=\\theta_j-\\eta(f_\\theta(x^{(k)})-y^{(k)})x_j^{(k)}$$\n",
    "**小批量梯度下降法**（mini-batch gradient descent）：使用随机的 m 个训练数据(索引集合为 K)更新参数。更新快，不易陷入局部最优解。\n",
    "$$\\theta_j:=\\theta_j-\\eta\\sum_{k \\in K}\\,(f_\\theta(x^{(k)})-y^{(k)})x_j^{(k)}$$\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 感知机模型（分类问题）\n",
    "只能解决线性可分的问题。\n",
    "二分类的决策边界（decision boundary分割线）是使权重（weight）向量 $\\bm w$（同之前$\\bm \\theta$） 成为法线向量的直线。（分割线与权重向量垂直）  \n",
    "直线的表达式为： $\\bm w \\cdot \\bm x =\\sum^n_{i=1}\\,w_ix_i=0$   （两个向量的内积）<br>\n",
    "或者： $\\bm w \\cdot \\bm x =|w|\\cdot |x|\\cdot \\cos \\theta =0\\;\\;\\;\\;\\;$   其中$\\;|w|\\;$ 为向量的长度\n",
    "判别函数 $f_w(x)=\\begin{cases} 1 & (w \\cdot x \\ge 0) \\\\ -1 & (w \\cdot x < 0) \\end{cases} $    \n",
    "决策边界权重向量一侧内积为正（判断为1）， 另一侧内积为负（判断为-1）  \n",
    "权重向量的更新表达式： $w:=\\begin{cases} w+y^{(i)}x^{(i)} & (f_w(x^{(i)}) \\neq y^{(i)}) \\\\ w & (f_w(x^{(i)}) =y^{(i)}) \\end{cases} \\;\\;\\;\\;$\n",
    "$y^{(i)}$ 值为 1 或 -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 逻辑回归\n",
    "把分类作为概率来考虑    \n",
    "概率函数： $f_\\theta(x)=\\frac 1 {1+exp(-\\bm \\theta^T \\bm x)} \\;\\;\\;\\;\\;$ 其中 sigmoid 函数为： $\\sigma (x)=\\frac 1 {1+exp(-x)} \\in (0,1)$    \n",
    "条件概率： $ P(y=1|\\bm x)= f_\\theta(\\bm x) \\;\\;\\;\\;\\;$    对于给出 x， y = 1 的概率\n",
    "判别函数为： $y=\\begin{cases} 1 & (f_\\theta(\\bm x) \\ge 0.5 \\; \\equiv \\bm \\theta^T \\bm x \\ge 0) \\\\ 0 & (f_\\theta(\\bm x)<0.5\\;\\equiv \\bm \\theta^T \\bm x< 0) \\end{cases} $    \n",
    "决策边界（decision boundary）：用于数据分类的直线/平面。此处为： $\\bm \\theta^T \\bm x= 0,\\;\\;$  其对应的 $f_\\theta(\\bm x)=0.5$    \n",
    "假定所有的训练数据都是互不影响、独立发生的，此时整体的概率可以用联合概率来表示(似然函数 likelihood)：\n",
    "$$L(\\bm \\theta)=\\prod^n_{i=1} P(y^{(i)}=1\\;|\\;x^{(i)})^{y^{(i)}}P(y^{(i)}=0\\;|\\;x^{(i)})^{1-y^{(i)}}$$\n",
    "对于似然函数取对数（对数函数单调递增，不影响）：$\\log L(\\bm \\theta)=\\log \\prod^n_{i=1} P(y^{(i)}=1\\;|\\;x^{(i)})^{y^{(i)}}P(y^{(i)}=0\\;|\\;x^{(i)})^{1-y^{(i)}}$    \n",
    "$$=\\sum^n_{i=1}\\,(\\log P(y^{(i)}=1\\;|\\;x^{(i)})^{y^{(i)}}+\\log P(y^{(i)}=0\\;|\\;x^{(i)})^{1-y^{(i)}})=\\sum^n_{i=1}\\,(y^{(i)}\\log P(y^{(i)}=1\\;|\\;x^{(i)})+(1-y^{(i)})\\log P(y^{(i)}=0\\;|\\;x^{(i)}))$$\n",
    "$$=\\sum^n_{i=1}\\,(y^{(i)}\\log P(y^{(i)}=1\\;|\\;x^{(i)})+(1-y^{(i)})\\log (1-P(y^{(i)}=1\\;|\\;x^{(i)})))=\\sum^n_{i=1}\\,(y^{(i)}\\log f_\\theta(x^{(i)})+(1-y^{(i)})\\log (1-f_\\theta(x^{(i)})))$$\n",
    "对似然函数进行偏微分：$\\frac{\\partial \\log L(\\bm \\theta)}{\\partial \\theta_j}=\\frac{\\partial}{\\partial \\theta_j}\\sum^n_{i=1}\\,(y^{(i)}\\log f_\\theta(x^{(i)})+(1-y^{(i)})\\log (1-f_\\theta(x^{(i)})))$   \n",
    "令 $u=\\log L(\\bm \\theta), \\;\\;\\;$ $v=f_\\theta(\\bm x), \\;\\;\\; \\frac{\\partial u}{\\partial \\theta_j}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_j}$   \n",
    "$$\\frac{\\partial u}{\\partial v}=\\frac{\\partial}{\\partial v}\\sum^n_{i=1}\\,(y^{(i)}\\log (v)+(1-y^{(i)})\\log (1-v))=\\sum^n_{i=1}\\,(\\frac {y^{(i)}} v-\\frac {1-y^{(i)}}{1-v})$$\n",
    "sigmoid 函数的微分为：$\\frac{\\partial \\sigma(x)}{\\partial x}= \\sigma(x)(1- \\sigma(x)), \\;\\;\\;$ 令 $z=\\bm \\theta^T \\bm x, \\;\\;\\;\\;\\; \\frac{\\partial v}{\\partial \\theta_j}=\\frac{\\partial v}{\\partial z}\\cdot\\frac{\\partial z}{\\partial \\theta_j}=v(1-v)\\frac{\\partial}{\\partial \\theta_j}(\\theta_0x_0+\\theta_1x_1+...+\\theta_nx_n)=v(1-v)\\cdot x_j$    \n",
    "$$\\frac{\\partial u}{\\partial \\theta_j}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_j}=\\sum^n_{i=1}\\,(\\frac {y^{(i)}} v-\\frac {1-y^{(i)}}{1-v})\\cdot v(1-v)\\cdot x_j^{(i)}=\\sum^n_{i=1}\\,(y^{(i)}-y^{(i)}v-v+y^{(i)}v)x_j^{(i)}=\\sum^n_{i=1}\\,(y^{(i)}-f_\\theta(x^{(i)}))x_j^{(i)}$$\n",
    "参数的更新表达式：$\\theta_j:=\\theta_j-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$   \n",
    "逻辑回归中处理线性不可分时， 只需要在原有一次向量 $\\bm x$ 中插入高次向，就可以使决策边界变为曲线/面，使得可以预测线性不可分问题。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## 感知机模型（分类问题）\n",
    "只能解决线性可分的问题。\n",
    "二分类的决策边界（decision boundary分割线）是使权重（weight）向量 $\\bm w$（同之前$\\bm \\theta$） 成为法线向量的直线。（分割线与权重向量垂直）  \n",
    "直线的表达式为： $\\bm w \\cdot \\bm x =\\sum^n_{i=1}\\,w_ix_i=0$   （两个向量的内积）<br>\n",
    "或者： $\\bm w \\cdot \\bm x =|w|\\cdot |x|\\cdot \\cos \\theta =0\\;\\;\\;\\;\\;$   其中$\\;|w|\\;$ 为向量的长度\n",
    "判别函数 $f_w(x)=\\begin{cases} 1 & (w \\cdot x \\ge 0) \\\\ -1 & (w \\cdot x < 0) \\end{cases} $    \n",
    "决策边界权重向量一侧内积为正（判断为1）， 另一侧内积为负（判断为-1）  \n",
    "权重向量的更新表达式： $w:=\\begin{cases} w+y^{(i)}x^{(i)} & (f_w(x^{(i)}) \\neq y^{(i)}) \\\\ w & (f_w(x^{(i)}) =y^{(i)}) \\end{cases} \\;\\;\\;\\;$\n",
    "$y^{(i)}$ 值为 1 或 -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归\n",
    "把分类作为概率来考虑    \n",
    "概率函数： $f_\\theta(x)=\\frac 1 {1+exp(-\\bm \\theta^T \\bm x)} \\;\\;\\;\\;\\;$ 其中 sigmoid 函数为： $\\sigma (x)=\\frac 1 {1+exp(-x)} \\in (0,1)$    \n",
    "条件概率： $ P(y=1|\\bm x)= f_\\theta(\\bm x) \\;\\;\\;\\;\\;$    对于给出 x， y = 1 的概率\n",
    "判别函数为： $y=\\begin{cases} 1 & (f_\\theta(\\bm x) \\ge 0.5 \\; \\equiv \\bm \\theta^T \\bm x \\ge 0) \\\\ 0 & (f_\\theta(\\bm x)<0.5\\;\\equiv \\bm \\theta^T \\bm x< 0) \\end{cases} $    \n",
    "决策边界（decision boundary）：用于数据分类的直线/平面。此处为： $\\bm \\theta^T \\bm x= 0,\\;\\;$  其对应的 $f_\\theta(\\bm x)=0.5$    \n",
    "假定所有的训练数据都是互不影响、独立发生的，此时整体的概率可以用联合概率来表示(似然函数 likelihood)：\n",
    "$$L(\\bm \\theta)=\\prod^n_{i=1} P(y^{(i)}=1\\;|\\;x^{(i)})^{y^{(i)}}P(y^{(i)}=0\\;|\\;x^{(i)})^{1-y^{(i)}}$$\n",
    "对于似然函数取对数（对数函数单调递增，不影响）：$\\log L(\\bm \\theta)=\\log \\prod^n_{i=1} P(y^{(i)}=1\\;|\\;x^{(i)})^{y^{(i)}}P(y^{(i)}=0\\;|\\;x^{(i)})^{1-y^{(i)}}$    \n",
    "$$=\\sum^n_{i=1}\\,(\\log P(y^{(i)}=1\\;|\\;x^{(i)})^{y^{(i)}}+\\log P(y^{(i)}=0\\;|\\;x^{(i)})^{1-y^{(i)}})=\\sum^n_{i=1}\\,(y^{(i)}\\log P(y^{(i)}=1\\;|\\;x^{(i)})+(1-y^{(i)})\\log P(y^{(i)}=0\\;|\\;x^{(i)}))$$\n",
    "$$=\\sum^n_{i=1}\\,(y^{(i)}\\log P(y^{(i)}=1\\;|\\;x^{(i)})+(1-y^{(i)})\\log (1-P(y^{(i)}=1\\;|\\;x^{(i)})))=\\sum^n_{i=1}\\,(y^{(i)}\\log f_\\theta(x^{(i)})+(1-y^{(i)})\\log (1-f_\\theta(x^{(i)})))$$\n",
    "对似然函数进行偏微分：$\\frac{\\partial \\log L(\\bm \\theta)}{\\partial \\theta_j}=\\frac{\\partial}{\\partial \\theta_j}\\sum^n_{i=1}\\,(y^{(i)}\\log f_\\theta(x^{(i)})+(1-y^{(i)})\\log (1-f_\\theta(x^{(i)})))$   \n",
    "令 $u=\\log L(\\bm \\theta), \\;\\;\\;$ $v=f_\\theta(\\bm x), \\;\\;\\; \\frac{\\partial u}{\\partial \\theta_j}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_j}$   \n",
    "$$\\frac{\\partial u}{\\partial v}=\\frac{\\partial}{\\partial v}\\sum^n_{i=1}\\,(y^{(i)}\\log (v)+(1-y^{(i)})\\log (1-v))=\\sum^n_{i=1}\\,(\\frac {y^{(i)}} v-\\frac {1-y^{(i)}}{1-v})$$\n",
    "sigmoid 函数的微分为：$\\frac{\\partial \\sigma(x)}{\\partial x}= \\sigma(x)(1- \\sigma(x)), \\;\\;\\;$ 令 $z=\\bm \\theta^T \\bm x, \\;\\;\\;\\;\\; \\frac{\\partial v}{\\partial \\theta_j}=\\frac{\\partial v}{\\partial z}\\cdot\\frac{\\partial z}{\\partial \\theta_j}=v(1-v)\\frac{\\partial}{\\partial \\theta_j}(\\theta_0x_0+\\theta_1x_1+...+\\theta_nx_n)=v(1-v)\\cdot x_j$    \n",
    "$$\\frac{\\partial u}{\\partial \\theta_j}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_j}=\\sum^n_{i=1}\\,(\\frac {y^{(i)}} v-\\frac {1-y^{(i)}}{1-v})\\cdot v(1-v)\\cdot x_j^{(i)}=\\sum^n_{i=1}\\,(y^{(i)}-y^{(i)}v-v+y^{(i)}v)x_j^{(i)}=\\sum^n_{i=1}\\,(y^{(i)}-f_\\theta(x^{(i)}))x_j^{(i)}$$\n",
    "参数的更新表达式：$\\theta_j:=\\theta_j-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$   \n",
    "逻辑回归中处理线性不可分时， 只需要在原有一次向量 $\\bm x$ 中插入高次向，就可以使决策边界变为曲线/面，使得可以预测线性不可分问题。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5ea5b284b387cf407f411a3436edf59739a2fcbca705f3a00844e9d18bc37be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}