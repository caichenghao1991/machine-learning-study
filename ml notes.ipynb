{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 线性回归（linear regression）\n",
    "定义回归模型（model）: $f_\\theta(x)=\\theta_0+\\theta_1x$  $\\;\\;\\;\\;\\;\\;$ 【一次线性模型，$\\theta_0$为斜率（slope）， $\\theta_0$为截距（intercept)】  \n",
    "其中参数（parameter）为 $\\theta$， x 为输入变量或输入特征(input variable/features)， y 为输出变量或目标变量(output variable/target variable)  \n",
    "目标函数（cost/objective function）: $$E(\\theta)=\\frac{1}{2}\\sum^n_{i=1}\\,(y^{(i)}-f_\\theta(x^{(i)}))^2   \\;\\;\\;\\;\\;\\;\\;\\;\\; L2正则项追加： +\\frac \\lambda 2 \\sum^m_{i=1}\\,\\theta_j^2   $$\n",
    "    **梯度下降法**（gradient descent）: 使用所有训练数据的误差令更新参数(求最小值时，原参数值减去偏微分乘以学习率)。对于多变量函数不能用普通的微分，而要用偏微分。其中 $\\eta$ 为学习率（learning rate）。存在耗时长，容易陷入局部最优解的缺点。\n",
    "    $$\\theta_0:=\\theta_0-\\eta\\frac{\\partial E}{\\partial \\theta_0}, \\;\\;\\;\\;\\; \\theta_1:=\\theta_1-\\eta\\frac{\\partial E}{\\partial \\theta_1}$$\n",
    "    令 $u=E(\\theta), \\;\\;\\;\\;\\; v=f_\\theta(x), \\;\\;\\;\\;\\; \\frac{\\partial u}{\\partial \\theta_0}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_0},$\n",
    "    $$\\frac{\\partial u}{\\partial v}=\\frac{\\partial }{\\partial v}(\\frac{1}{2}\\sum^n_{i=1}\\,(y^{(i)}-v)^2)=\\frac{1}{2}\\sum^n_{i=1}\\,(\\frac{\\partial }{\\partial v}\n",
    "    (y^{(i)^2}-2y^{(i)}v+v^2))=\\sum^n_{i=1}\\,(v-y^{(i)}),\\;\\;\\;\\;\\;  \\frac{\\partial v}{\\partial \\theta_0}=\\frac{\\partial }{\\partial \\theta_0}(\\theta_0+\\theta_1 x)=1$$\n",
    "    $$\\frac{\\partial u}{\\partial \\theta_0}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_0}=\\sum^n_{i=1}\\,(v-y^{(i)})=\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})$$\n",
    "    同理， $\\frac{\\partial v}{\\partial \\theta_1}=\\frac{\\partial }{\\partial \\theta_1}(\\theta_0+\\theta_1 x)=x, \\;\\;\\;\\;\\; \\frac{\\partial u}{\\partial \\theta_1}=$\n",
    "    $\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_1}=\\sum^n_{i=1}\\,(v-y^{(i)})\\cdot x^{(i)}=\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x^{(i)}$ \n",
    "    $$\\theta_0:=\\theta_0-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)}), \\;\\;\\;\\;\\; \\theta_1:=\\theta_1-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x^{(i)}$$\n",
    "\n",
    "**多项式回归**：$$f_\\theta(x)=\\theta_0+\\theta_1x+\\theta_2x^2+\\theta_3x^3+...+\\theta_n x^n  \\;\\;\\;\\;\\;【多项式模型】$$       \n",
    "$$同理，\\theta_j:=\\theta_j-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x^{(i)^j}   \\;\\;\\;\\;\\;\\;\\;L2正则项\\;(\\;j\\neq0时\\;)\\;追加：-\\eta\\lambda\\theta_j $$\n",
    "**多重回归**(多个变量)：\n",
    "\n",
    "$$f_\\theta(x)=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n=\\bm \\theta^T \\bm x, \\;\\;\\;\\;\\; 其中 \\; \\bm \\theta=\\begin{bmatrix}\n",
    "\\theta_0\\\\ \\theta_1\\\\ \\theta_2\\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\;\\;\\; \\bm x=\\begin{bmatrix}x_0\\\\ x_1\\\\ x_2\\\\ \\vdots \\\\ x_n \\end{bmatrix}\n",
    "\\;\\;\\; (x_0=1) $$\n",
    "$$\\frac{\\partial u}{\\partial \\theta_j}=\\frac{\\partial u}{\\partial v}\\cdot\\frac{\\partial v}{\\partial \\theta_j},  \\;\\;\\;\\;\\;\n",
    "\\frac{\\partial v}{\\partial \\theta_j}=\\frac{\\partial }{\\partial \\theta_j}(\\bm \\theta^T\\bm x)=x_j$$\n",
    "$$\\theta_j:=\\theta_j-\\eta\\sum^n_{i=1}\\,(f_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$\n",
    "**随机梯度下降法**（sochastic gradient descent）：使用随机的一个训练数据(索引为 k)更新参数。更新快，不易陷入局部最优解。\n",
    "$$\\theta_j:=\\theta_j-\\eta(f_\\theta(x^{(k)})-y^{(k)})x_j^{(k)}$$\n",
    "**小批量梯度下降法**（mini-batch gradient descent）：使用随机的 m 个训练数据(索引集合为 K)更新参数。更新快，不易陷入局部最优解。\n",
    "$$\\theta_j:=\\theta_j-\\eta\\sum_{k \\in K}\\,(f_\\theta(x^{(k)})-y^{(k)})x_j^{(k)}$$\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 感知机模型（分类问题）\n",
    "只能解决线性可分的问题。\n",
    "二分类的决策边界（decision boundary分割线）是使权重（weight）向量 $\\bm w$（同之前$\\bm \\theta$） 成为法线向量的直线。（分割线与权重向量垂直）  \n",
    "直线的表达式为： $\\bm w \\cdot \\bm x =\\sum^n_{i=1}\\,w_ix_i=0$   （两个向量的内积）<br>\n",
    "或者： $\\bm w \\cdot \\bm x =|w|\\cdot |x|\\cdot \\cos \\theta =0\\;\\;\\;\\;\\;$   其中$\\;|w|\\;$ 为向量的长度\n",
    "判别函数 $f_w(x)=\\begin{cases} 1 & (w \\cdot x \\ge 0) \\\\ -1 & (w \\cdot x < 0) \\end{cases} $    \n",
    "决策边界权重向量一侧内积为正（判断为1）， 另一侧内积为负（判断为-1）  \n",
    "权重向量的更新表达式： $w:=\\begin{cases} w+y^{(i)}x^{(i)} & (f_w(x^{(i)}) \\neq y^{(i)}) \\\\ w & (f_w(x^{(i)}) =y^{(i)}) \\end{cases} \\;\\;\\;\\;$\n",
    "$y^{(i)}$ 值为 1 或 -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5ea5b284b387cf407f411a3436edf59739a2fcbca705f3a00844e9d18bc37be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}